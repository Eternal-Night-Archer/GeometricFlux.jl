"""
    AbstractPositionalEncoding

Abstract type of positional encoding for GNN.
"""
abstract type AbstractPositionalEncoding end

"""
    RandomWalkPE{K}

Concrete type of positional encoding from random walk method.

See also [`positional_encode`](@ref) for generating positional encoding.
"""
struct RandomWalkPE{K} <: AbstractPositionalEncoding end

"""
    LaplacianPE{K}

Concrete type of positional encoding from graph Laplacian method.

See also [`positional_encode`](@ref) for generating positional encoding.
"""
struct LaplacianPE{K} <: AbstractPositionalEncoding end

"""
    positional_encode(RandomWalkPE{K}, A)

Returns positional encoding (PE) of size `(K, N)` where N is node number.
PE is generated by `K`-step random walk over given graph.

# Arguments

- `K::Int`: First dimension of PE.
- `A`: Adjacency matrix of a graph.

See also [`RandomWalkPE`](@ref) for random walk method.
"""
function positional_encode(::Type{RandomWalkPE{K}}, A::AbstractMatrix) where {K}
    N = size(A, 1)
    @assert K ≤ N "K=$K must less or equal to number of nodes ($N)"
    inv_D = GraphSignals.degree_matrix(A, Float32, inverse=true)

    RW = similar(A, size(A)..., K)
    RW[:, :, 1] .= A * inv_D
    for i in 2:K
        RW[:, :, i] .= RW[:, :, i-1] * RW[:, :, 1]
    end

    pe = similar(RW, K, N)
    for i in 1:N
        pe[:, i] .= RW[i, i, :]
    end

    return pe
end

"""
    positional_encode(LaplacianPE{K}, A)

Returns positional encoding (PE) of size `(K, N)` where `N` is node number.
PE is generated from eigenvectors of a graph Laplacian truncated by `K`.

# Arguments

- `K::Int`: First dimension of PE.
- `A`: Adjacency matrix of a graph.

See also [`LaplacianPE`](@ref) for graph Laplacian method.
"""
function positional_encode(::Type{LaplacianPE{K}}, A::AbstractMatrix) where {K}
    N = size(A, 1)
    @assert K ≤ N "K=$K must less or equal to number of nodes ($N)"
    L = GraphSignals.normalized_laplacian(A)
    U = eigvecs(L)
    return U[1:K, :]
end


"""
    EEquivGraphPE(in_dim=>out_dim; init=glorot_uniform, bias=true)

E(n)-equivariant positional encoding layer.

# Arguments

- `in_dim::Int`: dimension of input positional feature.
- `out_dim::Int`:  dimension of output positional feature.
- `init`: neural network initialization function.
- `bias::Bool`: dimension of edge feature.

# Examples

```jldoctest
julia> in_dim_edge, out_dim = 2, 5
(2, 5)

julia> l = EEquivGraphPE(in_dim_edge=>out_dim)
EEquivGraphPE(2 => 5)
```

See also [`EEquivGraphConv`](@ref).
"""
struct EEquivGraphPE{X} <: MessagePassing
    nn::X
end

function EEquivGraphPE(ch::Pair{Int,Int}; init=glorot_uniform, bias::Bool=true)
    in, out = ch
    nn = Flux.Dense(in, out; init=init, bias=bias)
    return EEquivGraphPE(nn)
end

@functor EEquivGraphPE

ϕ_x(l::EEquivGraphPE, m_ij) = l.nn(m_ij)

message(l::EEquivGraphPE, x_i, x_j, e) = (x_i - x_j) .* ϕ_x(l, e)

update(l::EEquivGraphPE, m::AbstractArray, x::AbstractArray) = m .+ x

# For variable graph
function(l::EEquivGraphPE)(fg::AbstractFeaturedGraph)
    X = node_feature(fg)
    E = edge_feature(fg)
    GraphSignals.check_num_nodes(fg, X)
    GraphSignals.check_num_nodes(fg, E)
    _, V, _ = propagate(l, graph(fg), E, X, nothing, mean, nothing, nothing)
    return ConcreteFeaturedGraph(fg, nf=V)
end

# For static graph
function(l::EEquivGraphPE)(el::NamedTuple, X::AbstractArray, E::AbstractArray)
    GraphSignals.check_num_nodes(el.N, X)
    # GraphSignals.check_num_edges(el.E, E)
    _, V, _ = propagate(l, el, E, X, nothing, mean, nothing, nothing)
    return V
end

(wg::WithGraph{<:EEquivGraphPE})(args...) = wg.layer(wg.graph, args...)

function Base.show(io::IO, l::EEquivGraphPE)
    print(io, "EEquivGraphPE(", input_dim(l), " => ", output_dim(l), ")")
end

input_dim(l::EEquivGraphPE) = size(l.nn.weight, 2)
output_dim(l::EEquivGraphPE) = size(l.nn.weight, 1)

positional_encode(wg::WithGraph{<:EEquivGraphPE}, args...) = wg(args...)
positional_encode(l::EEquivGraphPE, args...) = l(args...)

"""
    LSPE(fg, f_h, f_e, f_p, k; pe_method=RandomWalkPE)

Learnable structural positional encoding layer. `LSPE` layer can be seen as a GNN layer
warpped in `WithGraph`.

# Arguments

- `fg::FeaturedGraph`: A given graoh for positional encoding.
- `f_h::MessagePassing`: Neural network layer for node update.
- `f_e`: Neural network layer for edge update.
- `f_p`: Neural network layer for positional encoding.
- `k::Int`: Dimension of positional encoding.
- `pe_method`: Initializer for positional encoding.
"""
struct LSPE{H<:MessagePassing,E,F,P} <: AbstractPositionalEncoding
    f_h::H
    f_e::E
    f_p::F
    pe::P
end

function LSPE(fg::AbstractFeaturedGraph, f_h::MessagePassing, f_e, f_p, k::Int;
              pe_method=RandomWalkPE)
    A = GraphSignals.adjacency_matrix(fg)
    return LSPE(f_h, f_e, f_p, positional_encode(pe_method{k}, A))
end

# For variable graph
function (l::LSPE)(fg::AbstractFeaturedGraph)
    X = node_feature(fg)
    E = edge_feature(fg)
    GraphSignals.check_num_nodes(fg, X)
    GraphSignals.check_num_edges(fg, E)
    E, V = propagate(l, graph(fg), E, X)
    return ConcreteFeaturedGraph(fg, nf=V, ef=E)
end

# For static graph
function (l::LSPE)(el::NamedTuple, X::AbstractArray, E::AbstractArray)
    GraphSignals.check_num_nodes(el.N, X)
    GraphSignals.check_num_edges(el.E, E)
    E, V = propagate(l, graph(fg), E, X)
    return V, E
end

update_vertex(l::LSPE, el::NamedTuple, X, E::AbstractArray) = l.f_h(el, X, E)
update_vertex(l::LSPE, el::NamedTuple, X, E::Nothing) = l.f_h(el, X)

update_edge(l::LSPE, h_i, h_j, e_ij) = l.f_e(e_ij)

positional_encode(l::LSPE, p_i, p_j, e_ij) = l.f_p(p_i)

propagate(l::LSPE, sg::SparseGraph, E, V) = propagate(l, to_namedtuple(sg), E, V)

function propagate(l::LSPE, el::NamedTuple, E, V)
    e_ij = _gather(E, el.es)
    h_i = _gather(V, el.xs)
    h_j = _gather(V, el.nbrs)
    p_i = _gather(l.pe, el.xs)
    p_j = _gather(l.pe, el.nbrs)

    V = update_vertex(l, el, vcat(V, l.pe), E)
    E = update_edge(l, h_i, h_j, e_ij)
    l.pe = positional_encode(l, p_i, p_j, e_ij)
    return E, V
end

function Base.show(io::IO, l::LSPE)
    print(io, "LSPE(node_layer=", l.f_h)
    print(io, ", edge_layer=", l.f_e)
    print(io, ", positional_encode=", l.f_p, ")")
end
